{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Text Processing Storage\n",
    "\n",
    "> Standardized SQLite storage for text processing results with content hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-default-exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-showdoc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import sqlite3\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from cjm_plugin_system.utils.hashing import hash_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-row-docs",
   "metadata": {},
   "source": [
    "## TextProcessRow\n",
    "\n",
    "A dataclass representing a single row in the standardized `text_jobs` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-row-dataclass",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class TextProcessRow:\n",
    "    \"\"\"A single row from the text_jobs table.\"\"\"\n",
    "    job_id: str          # Unique job identifier\n",
    "    input_text: str      # Original input text\n",
    "    input_hash: str      # Hash of input text in \"algo:hexdigest\" format\n",
    "    spans: Optional[List[Dict[str, Any]]] = None  # Processed text spans\n",
    "    metadata: Optional[Dict[str, Any]] = None      # Processing metadata\n",
    "    created_at: Optional[float] = None              # Unix timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-row",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: job_id=job_abc123\n",
      "Input: Hello world. How are you?\n",
      "Spans: 2 spans\n"
     ]
    }
   ],
   "source": [
    "# Test TextProcessRow creation\n",
    "row = TextProcessRow(\n",
    "    job_id=\"job_abc123\",\n",
    "    input_text=\"Hello world. How are you?\",\n",
    "    input_hash=\"sha256:\" + \"a\" * 64,\n",
    "    spans=[\n",
    "        {\"text\": \"Hello world.\", \"start_char\": 0, \"end_char\": 12, \"label\": \"sentence\"},\n",
    "        {\"text\": \"How are you?\", \"start_char\": 13, \"end_char\": 25, \"label\": \"sentence\"}\n",
    "    ],\n",
    "    metadata={\"processor\": \"nltk\"}\n",
    ")\n",
    "\n",
    "print(f\"Row: job_id={row.job_id}\")\n",
    "print(f\"Input: {row.input_text}\")\n",
    "print(f\"Spans: {len(row.spans)} spans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-storage-docs",
   "metadata": {},
   "source": [
    "## TextProcessStorage\n",
    "\n",
    "Standardized SQLite storage that all text processing plugins should use. Defines the canonical schema for the `text_jobs` table with input hashing for traceability.\n",
    "\n",
    "**Schema:**\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS text_jobs (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    job_id TEXT UNIQUE NOT NULL,\n",
    "    input_text TEXT NOT NULL,\n",
    "    input_hash TEXT NOT NULL,\n",
    "    spans JSON,\n",
    "    metadata JSON,\n",
    "    created_at REAL NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "The `input_hash` column stores a hash of the input text in `\"algo:hexdigest\"` format, enabling downstream consumers to verify that the source text hasn't changed since processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-storage-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextProcessStorage:\n",
    "    \"\"\"Standardized SQLite storage for text processing results.\"\"\"\n",
    "\n",
    "    SCHEMA = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS text_jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            job_id TEXT UNIQUE NOT NULL,\n",
    "            input_text TEXT NOT NULL,\n",
    "            input_hash TEXT NOT NULL,\n",
    "            spans JSON,\n",
    "            metadata JSON,\n",
    "            created_at REAL NOT NULL\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    INDEX = \"CREATE INDEX IF NOT EXISTS idx_text_jobs_job_id ON text_jobs(job_id);\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str  # Absolute path to the SQLite database file\n",
    "    ):\n",
    "        \"\"\"Initialize storage and create table if needed.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            con.execute(self.SCHEMA)\n",
    "            con.execute(self.INDEX)\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        job_id: str,       # Unique job identifier\n",
    "        input_text: str,   # Original input text\n",
    "        input_hash: str,   # Hash of input text in \"algo:hexdigest\" format\n",
    "        spans: Optional[List[Dict[str, Any]]] = None,  # Processed text spans\n",
    "        metadata: Optional[Dict[str, Any]] = None       # Processing metadata\n",
    "    ) -> None:\n",
    "        \"\"\"Save a text processing result to the database.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            con.execute(\n",
    "                \"\"\"INSERT INTO text_jobs\n",
    "                   (job_id, input_text, input_hash, spans, metadata, created_at)\n",
    "                   VALUES (?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                (\n",
    "                    job_id,\n",
    "                    input_text,\n",
    "                    input_hash,\n",
    "                    json.dumps(spans) if spans else None,\n",
    "                    json.dumps(metadata) if metadata else None,\n",
    "                    time.time()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def get_by_job_id(\n",
    "        self,\n",
    "        job_id: str  # Job identifier to look up\n",
    "    ) -> Optional[TextProcessRow]:  # Row or None if not found\n",
    "        \"\"\"Retrieve a text processing result by job ID.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            cur = con.execute(\n",
    "                \"\"\"SELECT job_id, input_text, input_hash, spans, metadata, created_at\n",
    "                   FROM text_jobs WHERE job_id = ?\"\"\",\n",
    "                (job_id,)\n",
    "            )\n",
    "            row = cur.fetchone()\n",
    "            if not row:\n",
    "                return None\n",
    "            return TextProcessRow(\n",
    "                job_id=row[0],\n",
    "                input_text=row[1],\n",
    "                input_hash=row[2],\n",
    "                spans=json.loads(row[3]) if row[3] else None,\n",
    "                metadata=json.loads(row[4]) if row[4] else None,\n",
    "                created_at=row[5]\n",
    "            )\n",
    "\n",
    "    def list_jobs(\n",
    "        self,\n",
    "        limit: int = 100  # Maximum number of rows to return\n",
    "    ) -> List[TextProcessRow]:  # List of text processing rows\n",
    "        \"\"\"List text processing jobs ordered by creation time (newest first).\"\"\"\n",
    "        results = []\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            cur = con.execute(\n",
    "                \"\"\"SELECT job_id, input_text, input_hash, spans, metadata, created_at\n",
    "                   FROM text_jobs ORDER BY created_at DESC LIMIT ?\"\"\",\n",
    "                (limit,)\n",
    "            )\n",
    "            for row in cur:\n",
    "                results.append(TextProcessRow(\n",
    "                    job_id=row[0],\n",
    "                    input_text=row[1],\n",
    "                    input_hash=row[2],\n",
    "                    spans=json.loads(row[3]) if row[3] else None,\n",
    "                    metadata=json.loads(row[4]) if row[4] else None,\n",
    "                    created_at=row[5]\n",
    "                ))\n",
    "        return results\n",
    "\n",
    "    def verify_input(\n",
    "        self,\n",
    "        job_id: str  # Job identifier to verify\n",
    "    ) -> Optional[bool]:  # True if input matches, False if changed, None if not found\n",
    "        \"\"\"Verify the stored input text still matches its hash.\"\"\"\n",
    "        row = self.get_by_job_id(job_id)\n",
    "        if not row:\n",
    "            return None\n",
    "        current_hash = hash_bytes(row.input_text.encode())\n",
    "        return current_hash == row.input_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-testing-header",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage initialized at: /tmp/tmp62hw1v1n.db\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create storage with temp database\n",
    "tmp_db = tempfile.NamedTemporaryFile(suffix=\".db\", delete=False)\n",
    "storage = TextProcessStorage(tmp_db.name)\n",
    "\n",
    "print(f\"Storage initialized at: {tmp_db.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved job_test_001\n",
      "Input hash: sha256:1d473b202b6fea30ab890b153d9d5fa3a79830a7bdb6d662581a95bda1a57866\n"
     ]
    }
   ],
   "source": [
    "# Save a text processing result\n",
    "test_text = \"Hello world. How are you?\"\n",
    "input_hash = hash_bytes(test_text.encode())\n",
    "\n",
    "storage.save(\n",
    "    job_id=\"job_test_001\",\n",
    "    input_text=test_text,\n",
    "    input_hash=input_hash,\n",
    "    spans=[\n",
    "        {\"text\": \"Hello world.\", \"start_char\": 0, \"end_char\": 12, \"label\": \"sentence\"},\n",
    "        {\"text\": \"How are you?\", \"start_char\": 13, \"end_char\": 25, \"label\": \"sentence\"}\n",
    "    ],\n",
    "    metadata={\"processor\": \"nltk\", \"language\": \"english\"}\n",
    ")\n",
    "\n",
    "print(f\"Saved job_test_001\")\n",
    "print(f\"Input hash: {input_hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-retrieve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved: job_test_001\n",
      "Input: Hello world. How are you?\n",
      "Spans: 2 spans\n",
      "Input hash: sha256:1d473b202b6fea30ab890b1...\n",
      "get_by_job_id returns None for missing job: OK\n"
     ]
    }
   ],
   "source": [
    "# Retrieve by job ID\n",
    "row = storage.get_by_job_id(\"job_test_001\")\n",
    "assert row is not None\n",
    "assert row.job_id == \"job_test_001\"\n",
    "assert row.input_text == test_text\n",
    "assert row.input_hash == input_hash\n",
    "assert len(row.spans) == 2\n",
    "assert row.metadata[\"processor\"] == \"nltk\"\n",
    "assert row.created_at is not None\n",
    "\n",
    "print(f\"Retrieved: {row.job_id}\")\n",
    "print(f\"Input: {row.input_text}\")\n",
    "print(f\"Spans: {len(row.spans)} spans\")\n",
    "print(f\"Input hash: {row.input_hash[:30]}...\")\n",
    "\n",
    "# Missing job returns None\n",
    "assert storage.get_by_job_id(\"nonexistent\") is None\n",
    "print(\"get_by_job_id returns None for missing job: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-list",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_jobs returned 2 rows: ['job_test_002', 'job_test_001']\n"
     ]
    }
   ],
   "source": [
    "# Save another and test list_jobs\n",
    "storage.save(\n",
    "    job_id=\"job_test_002\",\n",
    "    input_text=\"Second text.\",\n",
    "    input_hash=hash_bytes(b\"Second text.\"),\n",
    "    spans=[{\"text\": \"Second text.\", \"start_char\": 0, \"end_char\": 12, \"label\": \"sentence\"}]\n",
    ")\n",
    "\n",
    "jobs = storage.list_jobs()\n",
    "assert len(jobs) == 2\n",
    "assert jobs[0].job_id == \"job_test_002\"  # Newest first\n",
    "\n",
    "print(f\"list_jobs returned {len(jobs)} rows: {[j.job_id for j in jobs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-verify",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify_input with unchanged text: True\n",
      "verify_input after tampering: False\n",
      "verify_input for missing job: None\n"
     ]
    }
   ],
   "source": [
    "# Test input verification\n",
    "assert storage.verify_input(\"job_test_001\") == True\n",
    "print(\"verify_input with unchanged text: True\")\n",
    "\n",
    "# Tamper with input text directly in DB\n",
    "with sqlite3.connect(tmp_db.name) as con:\n",
    "    con.execute(\"UPDATE text_jobs SET input_text = 'TAMPERED' WHERE job_id = 'job_test_001'\")\n",
    "\n",
    "assert storage.verify_input(\"job_test_001\") == False\n",
    "print(\"verify_input after tampering: False\")\n",
    "\n",
    "# Missing job returns None\n",
    "assert storage.verify_input(\"nonexistent\") is None\n",
    "print(\"verify_input for missing job: None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup complete\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "os.unlink(tmp_db.name)\n",
    "print(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-nbdev-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
